{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjh+1YmN7tHa+EOqUt7xsx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zakariasamy/8-puzzle/blob/master/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup and Introduction\n",
        "- This cell installs necessary libraries and provides an overview of the project."
      ],
      "metadata": {
        "id": "lJBQLPGtv6En"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0z4OUSp1PD-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab8a264-5e76-46e5-908b-2501f5e374dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "✅ All libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Setup and Introduction\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Final Thesis Implementation: Domain Knowledge guided Attentional Training\n",
        "Author: Zakaria Samy (with Gemini AI Assistant)\n",
        "Supervisor: Prof. Dr. Nahla Belal, Dr. Mohamed Seifelden\n",
        "\n",
        "This notebook implements and validates the core concepts of the thesis proposal.\n",
        "It is divided into two main parts:\n",
        "1.  Training and evaluating a standard CNN as a baseline.\n",
        "2.  Training and evaluating our novel Knowledge-Guided CNN.\n",
        "Finally, it runs a series of comparative \"stress tests\" to prove the superiority of the proposed framework.\n",
        "\"\"\"\n",
        "\n",
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Reshape\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"✅ All libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Data Loading and Preparation\n",
        "* This cell handles mounting your Google Drive, merging the 8 CSV files, and performing all the necessary preprocessing steps."
      ],
      "metadata": {
        "id": "r-jcbNp3wRyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Data Loading and Preparation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STAGE 1: DATA LOADING AND PREPARATION 📂\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_MOUNTED = True\n",
        "except ImportError:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Could not mount Google Drive. Assuming data is in the local Colab environment.\")\n",
        "\n",
        "# 2. Merge all CSV files into one DataFrame\n",
        "# --- IMPORTANT: Change this path to the exact folder you created in your Google Drive! ---\n",
        "drive_path = '/content/drive/MyDrive/Thesis_Dataset/'\n",
        "\n",
        "if DRIVE_MOUNTED and os.path.exists(drive_path):\n",
        "    all_files = glob.glob(os.path.join(drive_path, \"*.csv\"))\n",
        "else:\n",
        "    print(f\"Warning: Google Drive path '{drive_path}' not found. Looking for data in the current directory.\")\n",
        "    all_files = glob.glob(\"*.csv\")\n",
        "\n",
        "if not all_files:\n",
        "    raise ValueError(\"FATAL ERROR: No CSV files found. Please upload your dataset or check the 'drive_path' variable.\")\n",
        "\n",
        "list_of_dataframes = []\n",
        "for filename in all_files:\n",
        "    print(f'Reading {os.path.basename(filename)}...')\n",
        "    df = pd.read_csv(filename)\n",
        "    list_of_dataframes.append(df)\n",
        "\n",
        "print(\"\\nCombining all files...\")\n",
        "df = pd.concat(list_of_dataframes, ignore_index=True)\n",
        "print(f\"Initial dataset shape: {df.shape}\")\n",
        "\n",
        "# 3. Data Cleaning\n",
        "print(\"\\nStarting data cleaning...\")\n",
        "df.columns = df.columns.str.strip()\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Shape after cleaning: {df.shape}\")\n",
        "\n",
        "# 4. Feature and Label Separation\n",
        "X = df.drop('Label', axis=1)\n",
        "y_text = df['Label']\n",
        "\n",
        "# 5. Label Encoding\n",
        "print(\"\\nEncoding labels...\")\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_text)\n",
        "num_classes = len(le.classes_)\n",
        "print(f\"Found {num_classes} unique classes:\")\n",
        "for i, class_name in enumerate(le.classes_):\n",
        "    print(f\"  {i}: {class_name}\")\n",
        "\n",
        "y_categorical = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# 6. Feature Scaling\n",
        "print(\"\\nScaling features...\")\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# 7. Train-Test Split\n",
        "print(\"\\nSplitting data into training and testing sets (80/20)...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Convert to TensorFlow tensors for efficiency\n",
        "X_train_tf = tf.convert_to_tensor(X_train.values, dtype=tf.float32)\n",
        "X_test_tf = tf.convert_to_tensor(X_test.values, dtype=tf.float32)\n",
        "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "print(f\"Training features shape: {X_train_tf.shape}\")\n",
        "print(f\"Testing features shape: {X_test_tf.shape}\")\n",
        "print(\"\\n✅ Data preparation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmjcGz_-wWB2",
        "outputId": "0469a1f9-a6b9-43f0-9b91-fb20693d711b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STAGE 1: DATA LOADING AND PREPARATION 📂\n",
            "==================================================\n",
            "Mounted at /content/drive\n",
            "Reading Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...\n",
            "Reading Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...\n",
            "Reading Friday-WorkingHours-Morning.pcap_ISCX.csv...\n",
            "Reading Monday-WorkingHours.pcap_ISCX.csv...\n",
            "Reading Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv...\n",
            "Reading Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv...\n",
            "Reading Tuesday-WorkingHours.pcap_ISCX.csv...\n",
            "Reading Wednesday-workingHours.pcap_ISCX.csv...\n",
            "\n",
            "Combining all files...\n",
            "Initial dataset shape: (2830743, 79)\n",
            "\n",
            "Starting data cleaning...\n",
            "Shape after cleaning: (2520798, 79)\n",
            "\n",
            "Encoding labels...\n",
            "Found 15 unique classes:\n",
            "  0: BENIGN\n",
            "  1: Bot\n",
            "  2: DDoS\n",
            "  3: DoS GoldenEye\n",
            "  4: DoS Hulk\n",
            "  5: DoS Slowhttptest\n",
            "  6: DoS slowloris\n",
            "  7: FTP-Patator\n",
            "  8: Heartbleed\n",
            "  9: Infiltration\n",
            "  10: PortScan\n",
            "  11: SSH-Patator\n",
            "  12: Web Attack � Brute Force\n",
            "  13: Web Attack � Sql Injection\n",
            "  14: Web Attack � XSS\n",
            "\n",
            "Scaling features...\n",
            "\n",
            "Splitting data into training and testing sets (80/20)...\n",
            "Training features shape: (2016638, 78)\n",
            "Testing features shape: (504160, 78)\n",
            "\n",
            "✅ Data preparation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: Expert Knowledge Annotation (Simulated)\n",
        "* This cell simulates the output of your Stage 1 methodology. Based on a review of feature importance from multiple academic papers, we now define a much more detailed set of \"Knowledge Vectors.\""
      ],
      "metadata": {
        "id": "XhqHzxFgwflP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Expert Knowledge Annotation (Simulated - V2 with Deeper Knowledge)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STAGE 2: EXPERT KNOWLEDGE ANNOTATION (SIMULATED) 🧠\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "feature_names = X.columns.tolist()\n",
        "num_features = len(feature_names)\n",
        "default_vector = np.ones(num_features, dtype=np.float32)\n",
        "knowledge_vectors_np = {}\n",
        "\n",
        "def find_idx(name):\n",
        "    try:\n",
        "        return [c.strip() for c in feature_names].index(name)\n",
        "    except ValueError:\n",
        "        return -1 # Silently fail if feature not found\n",
        "\n",
        "# --- Create a more comprehensive knowledge base based on research ---\n",
        "\n",
        "# For DDoS & DoS Hulk (Flood Attacks)\n",
        "flood_vector = default_vector.copy()\n",
        "important_features = ['Total Fwd Packets', 'Fwd Packets/s', 'Bwd Packets/s', 'Average Packet Size', 'Packet Length Mean']\n",
        "unimportant_features = ['Fwd PSH Flags', 'Fwd URG Flags', 'Flow IAT Max', 'Flow IAT Min', 'FIN Flag Count']\n",
        "for fname in important_features: idx = find_idx(fname);\n",
        "if idx != -1: flood_vector[idx] = 10.0\n",
        "for fname in unimportant_features: idx = find_idx(fname);\n",
        "if idx != -1: flood_vector[idx] = 0.2\n",
        "knowledge_vectors_np['ddos'] = flood_vector\n",
        "knowledge_vectors_np['hulk'] = flood_vector\n",
        "knowledge_vectors_np['goldeneye'] = flood_vector\n",
        "\n",
        "# DoS Slowloris & Slowhttptest\n",
        "slow_vector = default_vector.copy()\n",
        "important_features = ['Flow Duration', 'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Std', 'Total Fwd Packets']\n",
        "unimportant_features = ['Fwd Packet Length Max', 'Average Packet Size', 'Packet Length Variance']\n",
        "for fname in important_features: idx = find_idx(fname);\n",
        "if idx != -1: slow_vector[idx] = 5.0\n",
        "for fname in unimportant_features: idx = find_idx(fname);\n",
        "if idx != -1: slow_vector[idx] = 0.2\n",
        "knowledge_vectors_np['slowloris'] = slow_vector\n",
        "knowledge_vectors_np['slowhttptest'] = slow_vector\n",
        "\n",
        "# PortScan\n",
        "portscan_vector = default_vector.copy()\n",
        "important_features = ['Destination Port', 'SYN Flag Count', 'FIN Flag Count', 'RST Flag Count', 'Flow IAT Mean']\n",
        "unimportant_features = ['Fwd Packet Length Max', 'Flow Duration', 'Total Fwd Packets', 'Idle Mean']\n",
        "for fname in important_features: idx = find_idx(fname);\n",
        "if idx != -1: portscan_vector[idx] = 5.0\n",
        "for fname in unimportant_features: idx = find_idx(fname);\n",
        "if idx != -1: portscan_vector[idx] = 0.2\n",
        "knowledge_vectors_np['portscan'] = portscan_vector\n",
        "\n",
        "# Brute Force (FTP/SSH/Web)\n",
        "brute_force_vector = default_vector.copy()\n",
        "important_features = ['Subflow Fwd Packets', 'Flow Packets/s', 'PSH Flag Count']\n",
        "misleading_features = ['Flow IAT Max', 'Idle Mean'] # Can be manipulated to look benign\n",
        "for fname in important_features: idx = find_idx(fname);\n",
        "if idx != -1: brute_force_vector[idx] = 3.0\n",
        "for fname in misleading_features: idx = find_idx(fname);\n",
        "if idx != -1: brute_force_vector[idx] = 0.1\n",
        "knowledge_vectors_np['patator'] = brute_force_vector\n",
        "knowledge_vectors_np['brute force'] = brute_force_vector\n",
        "\n",
        "# --- NEW VECTORS FOR RARE ATTACKS ---\n",
        "\n",
        "# Botnet\n",
        "# Logic: Botnets often use periodic, small \"heartbeat\" packets to a C&C server.\n",
        "bot_vector = default_vector.copy()\n",
        "important_features = ['Flow IAT Mean', 'Flow IAT Std', 'Idle Mean', 'Packet Length Mean']\n",
        "unimportant_features = ['Destination Port', 'Fwd Packet Length Max'] # Port can be common (80/443), Max length is irrelevant\n",
        "for fname in important_features: idx = find_idx(fname);\n",
        "if idx != -1: bot_vector[idx] = 4.0\n",
        "for fname in unimportant_features: idx = find_idx(fname);\n",
        "if idx != -1: bot_vector[idx] = 0.3\n",
        "knowledge_vectors_np['bot'] = bot_vector\n",
        "\n",
        "# Infiltration\n",
        "# Logic: Attacker is inside, trying to blend in. Look for long durations or unusual data transfers.\n",
        "infiltration_vector = default_vector.copy()\n",
        "important_features = ['Flow Duration', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets']\n",
        "misleading_features = ['Source Port', 'Destination Port'] # Often uses common ports to hide\n",
        "for fname in important_features: idx = find_idx(fname);\n",
        "if idx != -1: infiltration_vector[idx] = 4.0\n",
        "for fname in misleading_features: idx = find_idx(fname);\n",
        "if idx != -1: infiltration_vector[idx] = 0.1\n",
        "knowledge_vectors_np['infiltration'] = infiltration_vector\n",
        "\n",
        "# Web Attack - SQL Injection & XSS\n",
        "# Logic: The attack is in the packet payload, which we can't see. We must rely on secondary effects,\n",
        "# like an unusually large response from the server after the injection.\n",
        "web_app_attack_vector = default_vector.copy()\n",
        "important_features = ['Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Avg Bwd Segment Size']\n",
        "unimportant_features = ['Total Fwd Packets', 'Flow IAT Mean']\n",
        "for fname in important_features: idx = find_idx(fname);\n",
        "if idx != -1: web_app_attack_vector[idx] = 5.0\n",
        "for fname in unimportant_features: idx = find_idx(fname);\n",
        "if idx != -1: web_app_attack_vector[idx] = 0.3\n",
        "knowledge_vectors_np['sql injection'] = web_app_attack_vector\n",
        "knowledge_vectors_np['xss'] = web_app_attack_vector\n",
        "\n",
        "# --- Assign vectors to all classes ---\n",
        "final_knowledge_vectors = []\n",
        "for class_name in le.classes_:\n",
        "    vec_to_add = default_vector\n",
        "    assigned = False\n",
        "    for key, vec in knowledge_vectors_np.items():\n",
        "        if key.lower() in class_name.lower():\n",
        "            vec_to_add = vec\n",
        "            print(f\"Assigned specific knowledge vector to class '{class_name}'\")\n",
        "            assigned = True\n",
        "            break\n",
        "    if not assigned:\n",
        "        print(f\"Assigned default (neutral) vector to class '{class_name}'\")\n",
        "    final_knowledge_vectors.append(vec_to_add)\n",
        "\n",
        "knowledge_vectors_tf = tf.convert_to_tensor(np.array(final_knowledge_vectors), dtype=tf.float32)\n",
        "print(\"\\n✅ Comprehensive knowledge vectors created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZftY5nkVwtc_",
        "outputId": "b6422b0d-8458-4808-aecb-bb87e750df5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STAGE 2: EXPERT KNOWLEDGE ANNOTATION (SIMULATED) 🧠\n",
            "==================================================\n",
            "Assigned default (neutral) vector to class 'BENIGN'\n",
            "Assigned specific knowledge vector to class 'Bot'\n",
            "Assigned specific knowledge vector to class 'DDoS'\n",
            "Assigned specific knowledge vector to class 'DoS GoldenEye'\n",
            "Assigned specific knowledge vector to class 'DoS Hulk'\n",
            "Assigned specific knowledge vector to class 'DoS Slowhttptest'\n",
            "Assigned specific knowledge vector to class 'DoS slowloris'\n",
            "Assigned specific knowledge vector to class 'FTP-Patator'\n",
            "Assigned default (neutral) vector to class 'Heartbleed'\n",
            "Assigned specific knowledge vector to class 'Infiltration'\n",
            "Assigned specific knowledge vector to class 'PortScan'\n",
            "Assigned specific knowledge vector to class 'SSH-Patator'\n",
            "Assigned specific knowledge vector to class 'Web Attack � Brute Force'\n",
            "Assigned specific knowledge vector to class 'Web Attack � Sql Injection'\n",
            "Assigned specific knowledge vector to class 'Web Attack � XSS'\n",
            "\n",
            "✅ Comprehensive knowledge vectors created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Baseline Model Training and Evaluation\n",
        "* This cell defines, trains, and evaluates the standard CNN which will serve as our baseline for comparison."
      ],
      "metadata": {
        "id": "HwaHbkM1w2xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Baseline Model Training and Evaluation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PART 1: BASELINE MODEL IMPLEMENTATION (STANDARD TRAINING)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- HYPERPARAMETERS ---\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# --- CREATE WEIGHTS DIRECTORY ---\n",
        "weights_dir = '/content/drive/MyDrive/Thesis_Weights/'\n",
        "if DRIVE_MOUNTED:\n",
        "    os.makedirs(weights_dir, exist_ok=True)\n",
        "    print(f\"✅ Weights directory created/verified: {weights_dir}\")\n",
        "\n",
        "# 1.1 Model Architecture (Standard CNN)\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    \"\"\"Creates a standard 1D CNN model for NIDS.\"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Reshape((input_shape, 1)),\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 1.2 Training the Baseline Model\n",
        "print(\"\\n--- Training Baseline Model ---\")\n",
        "input_shape = X_train_tf.shape[1]\n",
        "baseline_model = create_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Check for existing baseline weights\n",
        "baseline_weights_path = os.path.join(weights_dir, 'baseline_model.weights.h5') if DRIVE_MOUNTED else 'baseline_model.weights.h5'\n",
        "if os.path.exists(baseline_weights_path):\n",
        "    print(f\"🔄 Loading existing baseline model weights from: {baseline_weights_path}\")\n",
        "    baseline_model.load_weights(baseline_weights_path)\n",
        "else:\n",
        "    print(\"📝 No existing baseline weights found. Training from scratch...\")\n",
        "    baseline_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "    baseline_history = baseline_model.fit(X_train_tf, y_train_tf, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test_tf, y_test_tf), verbose=1)\n",
        "\n",
        "    # Save the trained weights\n",
        "    baseline_model.save_weights(baseline_weights_path)\n",
        "    print(f\"💾 Baseline model weights saved to: {baseline_weights_path}\")\n",
        "\n",
        "# Ensure model is compiled for evaluation\n",
        "baseline_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# 1.3 Evaluating the Baseline Model\n",
        "print(\"\\n--- Evaluating Baseline Model on Full Test Set---\")\n",
        "y_pred_baseline = np.argmax(baseline_model.predict(X_test_tf), axis=1)\n",
        "y_true = np.argmax(y_test_tf.numpy(), axis=1)\n",
        "print(\"\\nClassification Report for Baseline Model:\")\n",
        "print(classification_report(y_true, y_pred_baseline, target_names=le.classes_))\n",
        "print(\"✅ Baseline evaluation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUeIIRehwy9X",
        "outputId": "d5ba2568-6976-493a-c108-782d3ad7e158"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PART 1: BASELINE MODEL IMPLEMENTATION (STANDARD TRAINING)\n",
            "================================================================================\n",
            "✅ Weights directory created/verified: /content/drive/MyDrive/Thesis_Weights/\n",
            "\n",
            "--- Training Baseline Model ---\n",
            "🔄 Loading existing baseline model weights from: /content/drive/MyDrive/Thesis_Weights/baseline_model.weights.h5\n",
            "\n",
            "--- Evaluating Baseline Model on Full Test Set---\n",
            "\u001b[1m15755/15755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step\n",
            "\n",
            "Classification Report for Baseline Model:\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       1.00      0.99      0.99    419012\n",
            "                       Bot       0.63      0.64      0.63       390\n",
            "                      DDoS       1.00      0.99      1.00     25603\n",
            "             DoS GoldenEye       0.99      0.98      0.98      2057\n",
            "                  DoS Hulk       0.99      0.98      0.99     34569\n",
            "          DoS Slowhttptest       0.98      1.00      0.99      1046\n",
            "             DoS slowloris       1.00      0.98      0.99      1077\n",
            "               FTP-Patator       0.99      1.00      0.99      1186\n",
            "                Heartbleed       1.00      0.50      0.67         2\n",
            "              Infiltration       1.00      0.14      0.25         7\n",
            "                  PortScan       0.83      0.99      0.91     18139\n",
            "               SSH-Patator       0.99      0.91      0.95       644\n",
            "  Web Attack � Brute Force       1.00      0.09      0.16       294\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         4\n",
            "          Web Attack � XSS       1.00      0.02      0.05       130\n",
            "\n",
            "                  accuracy                           0.99    504160\n",
            "                 macro avg       0.89      0.68      0.70    504160\n",
            "              weighted avg       0.99      0.99      0.99    504160\n",
            "\n",
            "✅ Baseline evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Knowledge-Guided Model Implementation and Training\n",
        "* This cell contains the implementation of your core innovation—the custom model with the knowledge-guided loss function—and then trains it."
      ],
      "metadata": {
        "id": "-1ULSldtw8Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Knowledge-Guided Model Implementation and Training (ERROR FIXED)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PART 2: KNOWLEDGE-GUIDED MODEL IMPLEMENTATION (OUR INNOVATION)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 2.1 The Custom Model and Loss Function\n",
        "class KnowledgeGuidedModel(Model):\n",
        "    def __init__(self, cnn_model, knowledge_vectors, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.knowledge_vectors = knowledge_vectors\n",
        "        self.alpha = alpha\n",
        "        self.cce_loss_fn = CategoricalCrossentropy()\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"Forward pass through the CNN model.\"\"\"\n",
        "        return self.cnn_model(inputs, training=training)\n",
        "\n",
        "    def compile(self, optimizer, metrics):\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    def get_knowledge_vectors_for_batch(self, y_true):\n",
        "        true_classes = tf.argmax(y_true, axis=1)\n",
        "        return tf.gather(self.knowledge_vectors, true_classes)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y_true = data\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(x)\n",
        "            y_pred = self.cnn_model(x, training=True)\n",
        "            cce_loss = self.cce_loss_fn(y_true, y_pred)\n",
        "\n",
        "            input_gradient = tape.gradient(cce_loss, x)\n",
        "            if input_gradient is None:\n",
        "                knowledge_loss = 0.0\n",
        "            else:\n",
        "                w = self.get_knowledge_vectors_for_batch(y_true)\n",
        "                knowledge_loss_per_feature = (1.0 / w) * tf.square(input_gradient)\n",
        "                knowledge_loss = tf.reduce_mean(tf.reduce_sum(knowledge_loss_per_feature, axis=1))\n",
        "\n",
        "            total_loss = cce_loss + self.alpha * knowledge_loss\n",
        "\n",
        "        trainable_vars = self.cnn_model.trainable_variables\n",
        "        weight_gradients = tape.gradient(total_loss, trainable_vars)\n",
        "        del tape\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(weight_gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y_true, y_pred)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({'loss': total_loss, 'cce_loss': cce_loss, 'knowledge_loss': knowledge_loss})\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # NOTE: For Keras `evaluate`, the compiled loss is what's returned as the 'loss' value.\n",
        "        # We will keep the custom train_step but simplify test_step.\n",
        "        x, y_true = data\n",
        "        y_pred = self.cnn_model(x, training=False)\n",
        "        # We calculate the standard CCE loss for evaluation purposes.\n",
        "        cce_loss = self.cce_loss_fn(y_true, y_pred)\n",
        "\n",
        "        # Update our compiled metrics (e.g., accuracy)\n",
        "        self.compiled_metrics.update_state(y_true, y_pred)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({'loss': cce_loss}) # Return standard loss during evaluation\n",
        "        return results\n",
        "\n",
        "# 2.2 Training the Knowledge-Guided Model\n",
        "print(\"\\n--- Training Knowledge-Guided Model ---\")\n",
        "ALPHA = 0.5 # Weight for the knowledge loss\n",
        "base_cnn_for_kg = create_cnn_model(input_shape, num_classes)\n",
        "knowledge_guided_model = KnowledgeGuidedModel(cnn_model=base_cnn_for_kg, knowledge_vectors=knowledge_vectors_tf, alpha=ALPHA)\n",
        "\n",
        "# Check for existing knowledge-guided weights\n",
        "kg_weights_path = os.path.join(weights_dir, 'knowledge_guided_model.weights.h5') if DRIVE_MOUNTED else 'knowledge_guided_model.weights.h5'\n",
        "if os.path.exists(kg_weights_path):\n",
        "    print(f\"🔄 Loading existing knowledge-guided model weights from: {kg_weights_path}\")\n",
        "    knowledge_guided_model.cnn_model.load_weights(kg_weights_path)\n",
        "else:\n",
        "    print(\"📝 No existing knowledge-guided weights found. Training from scratch...\")\n",
        "    knowledge_guided_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
        "    kg_history = knowledge_guided_model.fit(X_train_tf, y_train_tf, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test_tf, y_test_tf), verbose=1)\n",
        "\n",
        "    # Save the trained weights\n",
        "    knowledge_guided_model.cnn_model.save_weights(kg_weights_path)\n",
        "    print(f\"💾 Knowledge-guided model weights saved to: {kg_weights_path}\")\n",
        "\n",
        "# Ensure model is compiled for evaluation\n",
        "knowledge_guided_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
        "\n",
        "# 2.3 Evaluating the Knowledge-Guided Model\n",
        "print(\"\\n--- Evaluating Knowledge-Guided Model on Full Test Set ---\")\n",
        "\n",
        "# --- FIXED: Handle both list and dict return types ---\n",
        "results_kg = knowledge_guided_model.evaluate(X_test_tf, y_test_tf, verbose=0)\n",
        "if isinstance(results_kg, dict):\n",
        "    kg_loss = results_kg.get('loss', 0)\n",
        "    kg_acc = results_kg.get('accuracy', 0)\n",
        "else:\n",
        "    kg_loss = results_kg[0]\n",
        "    kg_acc = results_kg[1]\n",
        "\n",
        "# Ensure kg_acc is a number, not a tensor or dict\n",
        "if hasattr(kg_acc, 'numpy'):\n",
        "    kg_acc = float(kg_acc.numpy())\n",
        "elif isinstance(kg_acc, dict):\n",
        "    kg_acc = float(kg_acc.get('accuracy', 0))\n",
        "else:\n",
        "    kg_acc = float(kg_acc)\n",
        "\n",
        "print(f\"Knowledge-Guided Model Accuracy on Full Test Set: {kg_acc:.4f}\\n\")\n",
        "# --- END CORRECTION ---\n",
        "\n",
        "y_pred_kg = np.argmax(knowledge_guided_model.predict(X_test_tf), axis=1)\n",
        "print(\"\\nClassification Report for Knowledge-Guided Model:\")\n",
        "print(classification_report(y_true, y_pred_kg, target_names=le.classes_))\n",
        "print(\"✅ Knowledge-Guided evaluation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc1Dbw0Ew_8v",
        "outputId": "75732823-6445-4da2-a126-104cb1fa00b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PART 2: KNOWLEDGE-GUIDED MODEL IMPLEMENTATION (OUR INNOVATION)\n",
            "================================================================================\n",
            "\n",
            "--- Training Knowledge-Guided Model ---\n",
            "📝 No existing knowledge-guided weights found. Training from scratch...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7878/7878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 78ms/step - cce_loss: 0.0920 - accuracy: 0.9501 - knowledge_loss: 0.0030 - loss: 0.0935 - val_loss: 0.0297\n",
            "Epoch 2/5\n",
            "\u001b[1m7878/7878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 75ms/step - cce_loss: 0.0509 - accuracy: 0.9763 - knowledge_loss: 0.0069 - loss: 0.0544 - val_loss: 0.0173\n",
            "Epoch 3/5\n",
            "\u001b[1m7878/7878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 74ms/step - cce_loss: 0.0427 - accuracy: 0.9816 - knowledge_loss: 0.0095 - loss: 0.0475 - val_loss: 0.0124\n",
            "Epoch 4/5\n",
            "\u001b[1m7878/7878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 74ms/step - cce_loss: 0.0395 - accuracy: 0.9831 - knowledge_loss: 0.0102 - loss: 0.0446 - val_loss: 0.0069\n",
            "Epoch 5/5\n",
            "\u001b[1m7878/7878\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 73ms/step - cce_loss: 0.0376 - accuracy: 0.9835 - knowledge_loss: 0.0100 - loss: 0.0426 - val_loss: 0.0076\n",
            "💾 Knowledge-guided model weights saved to: /content/drive/MyDrive/Thesis_Weights/knowledge_guided_model.weights.h5\n",
            "\n",
            "--- Evaluating Knowledge-Guided Model on Full Test Set ---\n",
            "Knowledge-Guided Model Accuracy on Full Test Set: 0.9819\n",
            "\n",
            "\u001b[1m15755/15755\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step\n",
            "\n",
            "Classification Report for Knowledge-Guided Model:\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN       0.99      0.99      0.99    419012\n",
            "                       Bot       0.97      0.39      0.56       390\n",
            "                      DDoS       1.00      0.98      0.99     25603\n",
            "             DoS GoldenEye       0.99      0.98      0.98      2057\n",
            "                  DoS Hulk       1.00      0.95      0.97     34569\n",
            "          DoS Slowhttptest       0.89      0.98      0.94      1046\n",
            "             DoS slowloris       0.99      0.95      0.97      1077\n",
            "               FTP-Patator       0.96      0.99      0.98      1186\n",
            "                Heartbleed       1.00      0.50      0.67         2\n",
            "              Infiltration       1.00      0.14      0.25         7\n",
            "                  PortScan       0.77      0.97      0.86     18139\n",
            "               SSH-Patator       1.00      0.90      0.95       644\n",
            "  Web Attack � Brute Force       1.00      0.08      0.15       294\n",
            "Web Attack � Sql Injection       0.00      0.00      0.00         4\n",
            "          Web Attack � XSS       0.00      0.00      0.00       130\n",
            "\n",
            "                  accuracy                           0.98    504160\n",
            "                 macro avg       0.84      0.65      0.68    504160\n",
            "              weighted avg       0.98      0.98      0.98    504160\n",
            "\n",
            "✅ Knowledge-Guided evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Comparative Stress Tests\n",
        "- This final cell performs the comparative:\n",
        "- \"Stress Test A\" for ambiguous attacks\n",
        "- \"Stress Test B\" for The \"Few-Shot\" Generalization\n",
        "- \"Stress Test C\" for The Polymorphic Attack Test"
      ],
      "metadata": {
        "id": "_h-USQyCxEGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Comparative Stress Tests (All Phases Implemented)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PART 3: COMPARATIVE STRESS TESTS 🔬\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Stress Test A: The Ambiguous Attack Test ---\n",
        "print(\"\\n\\n--- Stress Test A (Ambiguous Attacks) ---\")\n",
        "try:\n",
        "    benign_idx = list(le.classes_).index('BENIGN')\n",
        "    hulk_idx = list(le.classes_).index('DoS Hulk')\n",
        "    slowloris_idx = list(le.classes_).index('DoS slowloris')\n",
        "    ambiguous_indices = [benign_idx, hulk_idx, slowloris_idx]\n",
        "\n",
        "    y_test_labels = np.argmax(y_test_tf.numpy(), axis=1)\n",
        "    mask = np.isin(y_test_labels, ambiguous_indices)\n",
        "\n",
        "    X_ambiguous_test = X_test_tf[mask]\n",
        "    y_ambiguous_test = y_test_tf[mask]\n",
        "    print(f\"Created ambiguous test set with {X_ambiguous_test.shape[0]} samples.\")\n",
        "\n",
        "    y_pred_baseline_amb = np.argmax(baseline_model.predict(X_ambiguous_test), axis=1)\n",
        "    y_true_ambiguous = np.argmax(y_ambiguous_test.numpy(), axis=1)\n",
        "\n",
        "    print(\"\\n--- Classification Report for Baseline Model (Ambiguous Test) ---\")\n",
        "    y_true_ambiguous_filtered = y_true_ambiguous[mask]\n",
        "    print(classification_report(y_true_ambiguous_filtered, y_pred_baseline_amb, labels=ambiguous_indices, target_names=[le.classes_[i] for i in ambiguous_indices], digits=4))\n",
        "\n",
        "    y_pred_kg_amb = np.argmax(knowledge_guided_model.predict(X_ambiguous_test), axis=1)\n",
        "    print(\"\\n--- Classification Report for Knowledge-Guided Model (Ambiguous Test) ---\")\n",
        "    print(classification_report(y_true_ambiguous_filtered, y_pred_kg_amb, labels=ambiguous_indices, target_names=[le.classes_[i] for i in ambiguous_indices], digits=4))\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"\\nCould not find a required attack class for Ambiguous Test. Skipping. Error: {e}\")\n",
        "\n",
        "\n",
        "# --- Stress Test B: The \"Few-Shot\" Generalization Test ---\n",
        "print(\"\\n\\n--- Stress Test B (Few-Shot Generalization) ---\")\n",
        "try:\n",
        "    # Use 'Web Attack  Brute Force' which is more common than Heartbleed\n",
        "    target_class_name = 'Web Attack  Brute Force'\n",
        "    target_class_idx = list(le.classes_).index(target_class_name)\n",
        "    num_shots = 20\n",
        "    print(f\"\\nCreating a few-shot training set for '{target_class_name}' with {num_shots} samples.\")\n",
        "\n",
        "    y_train_labels = np.argmax(y_train_tf.numpy(), axis=1)\n",
        "    target_indices = np.where(y_train_labels == target_class_idx)[0]\n",
        "\n",
        "    if len(target_indices) > num_shots:\n",
        "        np.random.shuffle(target_indices)\n",
        "        indices_to_keep = target_indices[:num_shots]\n",
        "        indices_to_remove = target_indices[num_shots:]\n",
        "\n",
        "        keep_mask = np.ones(len(X_train_tf), dtype=bool)\n",
        "        keep_mask[indices_to_remove] = False\n",
        "\n",
        "        X_train_fs = X_train_tf[keep_mask]\n",
        "        y_train_fs = y_train_tf[keep_mask]\n",
        "\n",
        "        print(f\"Original training set size: {len(X_train_tf)}\")\n",
        "        print(f\"Few-shot training set size: {len(X_train_fs)}\")\n",
        "        print(f\"Number of '{target_class_name}' samples in new training set: {np.sum(np.argmax(y_train_fs, axis=1) == target_class_idx)}\")\n",
        "\n",
        "        print(\"\\n--- Retraining Baseline Model on Few-Shot Data ---\")\n",
        "        baseline_model_fs = create_cnn_model(input_shape, num_classes)\n",
        "        baseline_model_fs.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "        baseline_model_fs.fit(X_train_fs, y_train_fs, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "        print(\"\\n--- Retraining Knowledge-Guided Model on Few-Shot Data ---\")\n",
        "        base_cnn_for_kg_fs = create_cnn_model(input_shape, num_classes)\n",
        "        kg_model_fs = KnowledgeGuidedModel(cnn_model=base_cnn_for_kg_fs, knowledge_vectors=knowledge_vectors_tf, alpha=ALPHA)\n",
        "        kg_model_fs.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
        "        kg_model_fs.fit(X_train_fs, y_train_fs, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "        print(\"\\n--- RESULTS FOR FEW-SHOT TEST ---\")\n",
        "        y_pred_baseline_fs = np.argmax(baseline_model_fs.predict(X_test_tf), axis=1)\n",
        "        print(\"\\nClassification Report for Baseline Model (Few-Shot Trained):\")\n",
        "        print(classification_report(y_true, y_pred_baseline_fs, target_names=le.classes_, digits=4))\n",
        "\n",
        "        y_pred_kg_fs = np.argmax(kg_model_fs.predict(X_test_tf), axis=1)\n",
        "        print(\"\\nClassification Report for Knowledge-Guided Model (Few-Shot Trained):\")\n",
        "        print(classification_report(y_true, y_pred_kg_fs, target_names=le.classes_, digits=4))\n",
        "    else:\n",
        "        print(f\"Not enough samples of '{target_class_name}' to run the few-shot test. Skipping.\")\n",
        "\n",
        "except ValueError:\n",
        "    print(f\"\\nCould not find the specified attack class for the Few-Shot test. Skipping.\")\n",
        "\n",
        "\n",
        "# --- Stress Test C: The Polymorphic Attack Test ---\n",
        "print(\"\\n\\n--- Stress Test C (Polymorphic Attacks) ---\")\n",
        "try:\n",
        "    ddos_class_idx = list(le.classes_).index('DDoS')\n",
        "    benign_class_idx = list(le.classes_).index('BENIGN')\n",
        "\n",
        "    y_test_labels = np.argmax(y_test_tf, axis=1)\n",
        "    ddos_mask = (y_test_labels == ddos_class_idx)\n",
        "    X_test_ddos = X_test_tf[ddos_mask]\n",
        "    y_test_ddos = y_test_tf[ddos_mask]\n",
        "\n",
        "    y_train_labels = np.argmax(y_train_tf, axis=1)\n",
        "    benign_mask_train = (y_train_labels == benign_class_idx)\n",
        "    X_train_benign_tf = X_train_tf[benign_mask_train]\n",
        "\n",
        "    if len(X_test_ddos) > 0 and len(X_train_benign_tf) > 0:\n",
        "        def generate_polymorphic_samples(x_attacks, y_attacks, x_benign, knowledge_vectors):\n",
        "            x_polymorphic = x_attacks.numpy().copy()\n",
        "            attack_labels = np.argmax(y_attacks.numpy(), axis=1)\n",
        "            w_batch = tf.gather(knowledge_vectors, attack_labels).numpy()\n",
        "            for i in range(len(x_polymorphic)):\n",
        "                w_sample = w_batch[i]\n",
        "                unimportant_indices = np.where(w_sample < 1.0)[0]\n",
        "                random_benign_sample = x_benign[np.random.randint(0, len(x_benign))]\n",
        "                x_polymorphic[i, unimportant_indices] = random_benign_sample.numpy()[unimportant_indices]\n",
        "            return tf.convert_to_tensor(x_polymorphic, dtype=tf.float32)\n",
        "\n",
        "        print(f\"Generating polymorphic variants for {len(X_test_ddos)} DDoS samples...\")\n",
        "        X_test_polymorphic = generate_polymorphic_samples(X_test_ddos, y_test_ddos, X_train_benign_tf, knowledge_vectors_tf)\n",
        "\n",
        "        print(\"\\n--- RESULTS FOR POLYMORPHIC TEST ---\")\n",
        "        y_pred_baseline_poly = np.argmax(baseline_model.predict(X_test_polymorphic), axis=1)\n",
        "        poly_baseline_acc = np.mean(y_pred_baseline_poly == ddos_class_idx)\n",
        "        print(f\"Baseline Model Accuracy on Polymorphic DDoS Test Set: {poly_baseline_acc:.4f}\")\n",
        "\n",
        "        y_pred_kg_poly = np.argmax(knowledge_guided_model.predict(X_test_polymorphic), axis=1)\n",
        "        poly_kg_acc = np.mean(y_pred_kg_poly == ddos_class_idx)\n",
        "        print(f\"Knowledge-Guided Model Accuracy on Polymorphic DDoS Test Set: {poly_kg_acc:.4f}\")\n",
        "    else:\n",
        "        print(\"Not enough DDoS or Benign samples to conduct the polymorphic test.\")\n",
        "\n",
        "except ValueError:\n",
        "    print(\"\\nCould not find 'DDoS' or 'BENIGN' classes for the Polymorphic Test. Skipping.\")\n",
        "\n",
        "print(\"\\n✅ Full evaluation notebook complete.\")"
      ],
      "metadata": {
        "id": "BDz3uvxGxITP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
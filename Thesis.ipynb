{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+5QK0GXD+oY4rv7C9iZK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zakariasamy/8-puzzle/blob/master/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z4OUSp1PD-w"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Thesis Implementation: Domain Knowledge guided Attentional Training\n",
        "Author: Zakaria Samy (with Gemini AI Assistant)\n",
        "Supervisor: Prof. Dr. Nahla Belal, Dr. Mohamed Seifelden\n",
        "\n",
        "This notebook implements and validates the core concepts of the thesis proposal.\n",
        "It compares a standard-trained CNN against our novel Knowledge-Guided CNN\n",
        "across a series of rigorous tests to prove the superiority of the proposed framework.\n",
        "\"\"\"\n",
        "\n",
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Reshape\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"âœ… All libraries imported successfully.\")\n",
        "\n",
        "################################################################################\n",
        "# STAGE 1: DATA LOADING AND PREPARATION\n",
        "################################################################################\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STAGE 1: DATA LOADING AND PREPARATION ðŸ“‚\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_MOUNTED = True\n",
        "except ImportError:\n",
        "    DRIVE_MOUNTED = False\n",
        "    print(\"Could not mount Google Drive. Assuming data is in the local Colab environment.\")\n",
        "\n",
        "# 2. Merge all CSV files into one DataFrame\n",
        "# --- IMPORTANT: Change this path to the exact folder you created in your Google Drive! ---\n",
        "drive_path = '/content/drive/MyDrive/Thesis_Dataset/'\n",
        "\n",
        "if DRIVE_MOUNTED and os.path.exists(drive_path):\n",
        "    all_files = glob.glob(os.path.join(drive_path, \"*.csv\"))\n",
        "else:\n",
        "    # Fallback for local execution if Drive is not available or path is wrong\n",
        "    print(f\"Warning: Google Drive path '{drive_path}' not found. Looking for data in the current directory.\")\n",
        "    all_files = glob.glob(\"*.csv\")\n",
        "\n",
        "\n",
        "if not all_files:\n",
        "    raise ValueError(\"FATAL ERROR: No CSV files found. Please upload your dataset or check the 'drive_path' variable.\")\n",
        "\n",
        "list_of_dataframes = []\n",
        "for filename in all_files:\n",
        "    print(f'Reading {os.path.basename(filename)}...')\n",
        "    df = pd.read_csv(filename)\n",
        "    list_of_dataframes.append(df)\n",
        "\n",
        "print(\"\\nCombining all files...\")\n",
        "df = pd.concat(list_of_dataframes, ignore_index=True)\n",
        "print(f\"Initial dataset shape: {df.shape}\")\n",
        "\n",
        "# 3. Data Cleaning\n",
        "print(\"\\nStarting data cleaning...\")\n",
        "df.columns = df.columns.str.strip()\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Shape after cleaning: {df.shape}\")\n",
        "\n",
        "# 4. Feature and Label Separation\n",
        "X = df.drop('Label', axis=1)\n",
        "y_text = df['Label']\n",
        "\n",
        "# 5. Label Encoding\n",
        "print(\"\\nEncoding labels...\")\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_text)\n",
        "num_classes = len(le.classes_)\n",
        "print(f\"Found {num_classes} unique classes:\")\n",
        "for i, class_name in enumerate(le.classes_):\n",
        "    print(f\"  {i}: {class_name}\")\n",
        "\n",
        "y_categorical = to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "# 6. Feature Scaling\n",
        "print(\"\\nScaling features...\")\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "# 7. Train-Test Split\n",
        "print(\"\\nSplitting data into training and testing sets (80/20)...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Convert to TensorFlow tensors for efficiency\n",
        "X_train_tf = tf.convert_to_tensor(X_train.values, dtype=tf.float32)\n",
        "X_test_tf = tf.convert_to_tensor(X_test.values, dtype=tf.float32)\n",
        "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "print(f\"Training features shape: {X_train_tf.shape}\")\n",
        "print(f\"Testing features shape: {X_test_tf.shape}\")\n",
        "print(\"\\nâœ… Data preparation complete.\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# STAGE 2: EXPERT KNOWLEDGE ANNOTATION (SIMULATED)\n",
        "################################################################################\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STAGE 2: EXPERT KNOWLEDGE ANNOTATION (SIMULATED) ðŸ§ \")\n",
        "print(\"=\"*50)\n",
        "\n",
        "feature_names = X.columns.tolist()\n",
        "num_features = len(feature_names)\n",
        "\n",
        "# Helper function to find feature index safely\n",
        "def find_idx(name):\n",
        "    try:\n",
        "        return feature_names.index(name)\n",
        "    except ValueError:\n",
        "        print(f\"Warning: Feature '{name}' not found. Using default index.\")\n",
        "        return -1 # Return an invalid index to be handled\n",
        "\n",
        "# Create Knowledge Vectors\n",
        "knowledge_vectors_np = {}\n",
        "default_vector = np.ones(num_features, dtype=np.float32)\n",
        "\n",
        "# DoS Hulk Vector\n",
        "hulk_vector = default_vector.copy()\n",
        "idx = find_idx('Total Fwd Packets')\n",
        "if idx != -1: hulk_vector[idx] = 10.0\n",
        "knowledge_vectors_np['DoS Hulk'] = hulk_vector\n",
        "\n",
        "# DoS Slowloris Vector\n",
        "slowloris_vector = default_vector.copy()\n",
        "idx1 = find_idx('Total Fwd Packets')\n",
        "idx2 = find_idx('Flow Duration')\n",
        "if idx1 != -1: slowloris_vector[idx1] = 2.0\n",
        "if idx2 != -1: slowloris_vector[idx2] = 5.0\n",
        "knowledge_vectors_np['DoS slowloris'] = slowloris_vector\n",
        "\n",
        "# SSH-Patator (Brute Force) Vector\n",
        "ssh_vector = default_vector.copy()\n",
        "idx = find_idx('Flow IAT Max')\n",
        "if idx != -1: ssh_vector[idx] = 0.1 # Actively misleading\n",
        "knowledge_vectors_np['SSH-Patator'] = ssh_vector\n",
        "\n",
        "# Create the final TF tensor list for the model\n",
        "# The order MUST match the order of le.classes_\n",
        "final_knowledge_vectors = []\n",
        "for class_name in le.classes_:\n",
        "    # Find a matching key, handling variations like 'DoS slowloris' vs 'slowloris'\n",
        "    vec_to_add = default_vector\n",
        "    for key, vec in knowledge_vectors_np.items():\n",
        "        if key.lower() in class_name.lower():\n",
        "            vec_to_add = vec\n",
        "            print(f\"Assigned specific knowledge vector to class '{class_name}'\")\n",
        "            break\n",
        "    final_knowledge_vectors.append(vec_to_add)\n",
        "\n",
        "knowledge_vectors_tf = tf.convert_to_tensor(np.array(final_knowledge_vectors), dtype=tf.float32)\n",
        "print(\"\\nâœ… Knowledge vectors created and converted to TensorFlow tensor.\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# STAGE 3: MODEL DEFINITION AND CUSTOM LOSS\n",
        "################################################################################\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STAGE 3: MODEL DEFINITION AND CUSTOM LOSS âš™ï¸\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 3.1 Model Architecture (Standard CNN)\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    \"\"\"Creates a standard 1D CNN model for NIDS.\"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_shape,)),\n",
        "        Reshape((input_shape, 1)),\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 3.2 The Custom Model and Loss Function\n",
        "class KnowledgeGuidedModel(Model):\n",
        "    \"\"\"A custom Keras model that implements the DK-GAT framework.\"\"\"\n",
        "    def __init__(self, cnn_model, knowledge_vectors, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.knowledge_vectors = knowledge_vectors\n",
        "        self.alpha = alpha\n",
        "        self.cce_loss_fn = CategoricalCrossentropy()\n",
        "\n",
        "    def compile(self, optimizer, metrics):\n",
        "        super().compile(optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    def get_knowledge_vectors_for_batch(self, y_true):\n",
        "        true_classes = tf.argmax(y_true, axis=1)\n",
        "        return tf.gather(self.knowledge_vectors, true_classes)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y_true = data\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(x)\n",
        "            y_pred = self.cnn_model(x, training=True)\n",
        "            cce_loss = self.cce_loss_fn(y_true, y_pred)\n",
        "\n",
        "            # --- Innovation: Knowledge-Guided Loss ---\n",
        "            input_gradient = tape.gradient(cce_loss, x)\n",
        "            if input_gradient is None:\n",
        "                knowledge_loss = 0.0\n",
        "            else:\n",
        "                w = self.get_knowledge_vectors_for_batch(y_true)\n",
        "                knowledge_loss_per_feature = (1.0 / w) * tf.square(input_gradient)\n",
        "                knowledge_loss = tf.reduce_mean(tf.reduce_sum(knowledge_loss_per_feature, axis=1))\n",
        "\n",
        "            total_loss = cce_loss + self.alpha * knowledge_loss\n",
        "            # --- End Innovation ---\n",
        "\n",
        "        trainable_vars = self.cnn_model.trainable_variables\n",
        "        weight_gradients = tape.gradient(total_loss, trainable_vars)\n",
        "        del tape # Drop the persistent tape\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(weight_gradients, trainable_vars))\n",
        "        self.compiled_metrics.update_state(y_true, y_pred)\n",
        "\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({'loss': total_loss, 'cce_loss': cce_loss, 'knowledge_loss': knowledge_loss})\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Override test_step to only calculate the standard loss for evaluation\n",
        "        x, y_true = data\n",
        "        y_pred = self.cnn_model(x, training=False)\n",
        "        cce_loss = self.cce_loss_fn(y_true, y_pred)\n",
        "        self.compiled_metrics.update_state(y_true, y_pred)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({'loss': cce_loss})\n",
        "        return results\n",
        "\n",
        "print(\"âœ… Model architecture and custom loss function defined.\")\n",
        "\n",
        "################################################################################\n",
        "# STAGE 4: EXPERIMENTAL EVALUATION\n",
        "################################################################################\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STAGE 4: EXPERIMENTAL EVALUATION ðŸ“Š\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- HYPERPARAMETERS ---\n",
        "EPOCHS = 5 # For a quick demo. For real results, use a larger number (e.g., 50).\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 0.001\n",
        "ALPHA = 0.5 # Weight for the knowledge loss\n",
        "\n",
        "# --- PHASE 1: BASELINE PERFORMANCE VALIDATION ---\n",
        "print(\"\\n--- PHASE 1: Training and Evaluating Baseline Model ---\")\n",
        "baseline_model = create_cnn_model(X_train_tf.shape[1], num_classes)\n",
        "baseline_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "baseline_model.fit(X_train_tf, y_train_tf, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test_tf, y_test_tf), verbose=1)\n",
        "\n",
        "print(\"\\n--- PHASE 1: Training and Evaluating Knowledge-Guided Model ---\")\n",
        "base_cnn_for_kg = create_cnn_model(X_train_tf.shape[1], num_classes)\n",
        "knowledge_guided_model = KnowledgeGuidedModel(cnn_model=base_cnn_for_kg, knowledge_vectors=knowledge_vectors_tf, alpha=ALPHA)\n",
        "knowledge_guided_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), metrics=['accuracy'])\n",
        "knowledge_guided_model.fit(X_train_tf, y_train_tf, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test_tf, y_test_tf), verbose=1)\n",
        "\n",
        "print(\"\\n\\n--- RESULTS FOR PHASE 1 (Full Test Set) ---\")\n",
        "_, baseline_acc = baseline_model.evaluate(X_test_tf, y_test_tf, verbose=0)\n",
        "print(f\"Baseline Model Accuracy on Full Test Set: {baseline_acc:.4f}\")\n",
        "_, kg_acc = knowledge_guided_model.evaluate(X_test_tf, y_test_tf, verbose=0)\n",
        "print(f\"Knowledge-Guided Model Accuracy on Full Test Set: {kg_acc:.4f}\")\n",
        "\n",
        "\n",
        "# --- PHASE 2: STRESS TEST A - THE AMBIGUOUS ATTACK TEST ---\n",
        "print(\"\\n\\n--- PHASE 2: Stress Test A (Ambiguous Attacks) ---\")\n",
        "try:\n",
        "    benign_idx = list(le.classes_).index('BENIGN')\n",
        "    hulk_idx = list(le.classes_).index('DoS Hulk')\n",
        "    slowloris_idx = list(le.classes_).index('DoS slowloris')\n",
        "    ambiguous_indices = [benign_idx, hulk_idx, slowloris_idx]\n",
        "\n",
        "    y_test_labels = np.argmax(y_test_tf.numpy(), axis=1)\n",
        "    mask = np.isin(y_test_labels, ambiguous_indices)\n",
        "\n",
        "    X_ambiguous_test = X_test_tf[mask]\n",
        "    y_ambiguous_test = y_test_tf[mask]\n",
        "    print(f\"Created ambiguous test set with {X_ambiguous_test.shape[0]} samples.\")\n",
        "\n",
        "    y_pred_baseline = np.argmax(baseline_model.predict(X_ambiguous_test), axis=1)\n",
        "    y_true_ambiguous = np.argmax(y_ambiguous_test.numpy(), axis=1)\n",
        "    print(\"\\n--- Classification Report for Baseline Model (Ambiguous Test) ---\")\n",
        "    print(classification_report(y_true_ambiguous, y_pred_baseline, labels=ambiguous_indices, target_names=[le.classes_[i] for i in ambiguous_indices]))\n",
        "\n",
        "    y_pred_kg = np.argmax(knowledge_guided_model.predict(X_ambiguous_test), axis=1)\n",
        "    print(\"\\n--- Classification Report for Knowledge-Guided Model (Ambiguous Test) ---\")\n",
        "    print(classification_report(y_true_ambiguous, y_pred_kg, labels=ambiguous_indices, target_names=[le.classes_[i] for i in ambiguous_indices]))\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"\\nCould not find one of the required attack classes for Ambiguous Test. Skipping. Error: {e}\")\n",
        "\n",
        "# --- PHASE 3: STRESS TEST B - \"FEW-SHOT\" GENERALIZATION TEST ---\n",
        "# This part can be computationally intensive and is left as an exercise for the full thesis implementation.\n",
        "# The code would be similar to what was provided in the previous text-based response.\n",
        "print(\"\\n\\n--- PHASE 3: Stress Test B (Few-Shot Generalization) ---\")\n",
        "print(\"Skipping for this demonstration, but the code is available in the chat history.\")\n",
        "print(\"The goal is to retrain both models on a dataset with a drastically reduced number of samples for one attack class and test on the full test set.\")\n",
        "\n",
        "\n",
        "# --- PHASE 4: STRESS TEST C - POLYMORPHIC ATTACK TEST ---\n",
        "print(\"\\n\\n--- PHASE 4: Stress Test C (Polymorphic Attacks) ---\")\n",
        "try:\n",
        "    ddos_class_idx = list(le.classes_).index('DDoS')\n",
        "    benign_class_idx = list(le.classes_).index('BENIGN')\n",
        "\n",
        "    # Get all DDoS samples from the test set\n",
        "    y_test_labels = np.argmax(y_test_tf, axis=1)\n",
        "    ddos_mask = (y_test_labels == ddos_class_idx)\n",
        "    X_test_ddos = X_test_tf[ddos_mask]\n",
        "    y_test_ddos = y_test_tf[ddos_mask]\n",
        "\n",
        "    # Get all Benign samples from the training set (to borrow from)\n",
        "    y_train_labels = np.argmax(y_train_tf, axis=1)\n",
        "    benign_mask_train = (y_train_labels == benign_class_idx)\n",
        "    X_train_benign_tf = X_train_tf[benign_mask_train]\n",
        "\n",
        "    if len(X_test_ddos) > 0 and len(X_train_benign_tf) > 0:\n",
        "        def generate_polymorphic_samples(x_attacks, y_attacks, x_benign, knowledge_vectors):\n",
        "            x_polymorphic = x_attacks.numpy().copy()\n",
        "            attack_labels = np.argmax(y_attacks.numpy(), axis=1)\n",
        "            w_batch = tf.gather(knowledge_vectors, attack_labels).numpy()\n",
        "            for i in range(len(x_polymorphic)):\n",
        "                w_sample = w_batch[i]\n",
        "                unimportant_indices = np.where(w_sample < 1.0)[0]\n",
        "                random_benign_sample = x_benign[np.random.randint(0, len(x_benign))]\n",
        "                x_polymorphic[i, unimportant_indices] = random_benign_sample.numpy()[unimportant_indices]\n",
        "            return tf.convert_to_tensor(x_polymorphic, dtype=tf.float32)\n",
        "\n",
        "        print(f\"Generating polymorphic variants for {len(X_test_ddos)} DDoS samples...\")\n",
        "        X_test_polymorphic = generate_polymorphic_samples(X_test_ddos, y_test_ddos, X_train_benign_tf, knowledge_vectors_tf)\n",
        "\n",
        "        print(\"\\n--- RESULTS FOR PHASE 4 (Polymorphic Evaluation) ---\")\n",
        "        _, baseline_poly_acc = baseline_model.evaluate(X_test_polymorphic, y_test_ddos, verbose=0)\n",
        "        print(f\"Baseline Model Accuracy on Polymorphic DDoS Test Set: {baseline_poly_acc:.4f}\")\n",
        "\n",
        "        _, kg_poly_acc = knowledge_guided_model.evaluate(X_test_polymorphic, y_test_ddos, verbose=0)\n",
        "        print(f\"Knowledge-Guided Model Accuracy on Polymorphic DDoS Test Set: {kg_poly_acc:.4f}\")\n",
        "    else:\n",
        "        print(\"Not enough DDoS or Benign samples to conduct the polymorphic test.\")\n",
        "\n",
        "except ValueError:\n",
        "    print(\"\\nCould not find 'DDoS' or 'BENIGN' classes for the Polymorphic Test. Skipping.\")\n",
        "\n",
        "print(\"\\nâœ… Full evaluation complete.\")@"
      ]
    }
  ]
}